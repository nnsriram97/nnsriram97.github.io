<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">

<font size="5">
<title>
Home &#8211; Sriram Narayanan
</title>
</font>


<meta name="keywords" content="">


<!-- Twitter Cards -->
<meta name="twitter:title" content="Home">

<meta name="twitter:site" content="@nnsriram97">
<meta name="twitter:creator" content="@nnsriram97">

<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://localhost:4002/images/default-thumb.png">

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Home">

<meta property="og:url" content="http://localhost:4002/">
<meta property="og:site_name" content="Sriram Narayanan">





<link rel="canonical" href="http://localhost:4002/">
<link href="http://localhost:4002/feed.xml" type="application/atom+xml" rel="alternate" title="Sriram Narayanan Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4002/assets/css/main.css">

<meta http-equiv="cleartype" content="on">

<!-- HTML5 Shiv and Media Query Support -->
<!--[if lt IE 9]>
	<script src="http://localhost:4002/assets/js/vendor/html5shiv.min.js"></script>
	<script src="http://localhost:4002/assets/js/vendor/respond.min.js"></script>
<![endif]-->

<!-- Modernizr -->
<script src="http://localhost:4002/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>

<link href='//fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700%7CPT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://localhost:4002/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://localhost:4002/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://localhost:4002/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://localhost:4002/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://localhost:4002/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://localhost:4002/images/apple-touch-icon-144x144-precomposed.png">

</head>

<body class="page">

<!--[if lt IE 9]><div class="browser-upgrade alert alert-info">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->

<div class="navigation-wrapper">
	<div class="site-name">
		<a href="http://localhost:4002">Sriram Narayanan</a>
	</div><!-- /.site-name -->
	<div class="top-navigation">
		<nav role="navigation" id="site-nav" class="nav">
		    <ul>
		        
				    
				    <li><a href="http://localhost:4002/" >Home</a></li>
				
				    
				    <li><a href="http://localhost:4002/Projects" >Projects</a></li>
				
				    
				    <li><a href="https://drive.google.com/file/d/0BycMB_n4CcSTVWtScDl0RS1Uc1E/view?usp=sharing&resourcekey=0-Rlq-5yferWs-DwTS75jzQw" target="_blank">CV</a></li>
				
		    </ul>
		</nav>
	</div><!-- /.top-navigation -->
</div><!-- /.navigation-wrapper -->



<div id="main" role="main">
  <div class="article-author-side">
    

<div itemscope itemtype="http://schema.org/Person">


	<img src="http://localhost:4002/images/bio-photo-3.jpg" class="bio-photo" alt=" bio photo">


  <h3 itemprop="name"></h3>  

  <p>PhD Student, Carnegie Mellon University</p>


  <a href="mailto:nnsriram97@gmail.com" class="author-social" target="_blank"><i class="fa fa-fw fa-envelope-square"></i>Email</a>
  <a href="http://twitter.com/nnsriram97" class="author-social" target="_blank"><i class="fa fa-fw fa-twitter-square"></i> Twitter</a>
  
  
  <a href="http://linkedin.com/in/sriram-n-n-777a0010b" class="author-social" target="_blank"><i class="fa fa-fw fa-linkedin-square"></i> LinkedIn</a>
  
  
  
  
  
  
  
  
  
  
  
  
  <a href="https://scholar.google.com/citations?user=P0CpOFwAAAAJ&hl=en" class="author-social" target="_blank"><i class="fa fa-fw fa-scholar"></i> Scholar</a>
</div>  
  </div>
  <article class="page">
    <!--<h2>Home</h2>-->
    <div class="article-wrap">
      <style>
a.paper:link, a.paper:visited {
  background-color: #E95D16;
  color: white;
  padding: 5px 5px;
  text-align: center;
  text-decoration: none;
  display: inline-block;
}

a.paper:hover, a.paper:active {
  background-color: red;
}
a.links:link, a.links:visited {
  background-color: none;
  color: blue;
  border-style: solid;
  border-width: thin;
  border-color: blue;
  padding: 3px 3px;
  text-align: center;
  text-decoration: none;
  display: inline-block;
}

a.links:hover, a.links:active {
  background-color: blue;
  color: white;
  border-style: solid;
  border-color: black;
}

a.genlinks:link, a.genlinks:visited {
  background-color: none;
  color: #66B2FF;
  display: inline-block;
}

/*Block Quote CSS Styles below*/
@import url(https://fonts.googleapis.com/css?family=EB+Garamond|Droid+Serif|Playfair+Display|Open+Sans+Condensed:300);

*, *:after, *:before {
    -webkit-box-sizing: border-box;
       -moz-box-sizing: border-box;
            box-sizing: border-box;
}

p {
    line-height: 1.4em;
}

.quote {
    position: relative;
    letter-spacing: .03em;
    margin-bottom: .5rem;
    
    &:before {
        content: "“";
        position: absolute;
        left: -.7em;
    }
    
    &:after {
        content: "”";
        margin-right: -1rem;
    }
}

.quote--container {
    margin: 3.5rem auto 0;
    width: 55%;
    /*border-bottom: 2px dotted #C6D1BF;*/
    padding-bottom: .5rem;
}

.quote--highlight {
    color: #D24335;
}

.quote--author {
    font-family: 'Open Sans Condensed';
    font-size: .8rem;
    text-align: right;
    font-weight: 300;
}


</style>

<p>Hi, I am a Ph.D. student at the <a href="https://www.ri.cmu.edu/">Robotics Institute</a> , <a href="https://www.cs.cmu.edu/">School of Computer Science</a>, <a href="https://www.cs.cmu.edu/">Carnegie Mellon University</a> working with <a href="http://www.cs.cmu.edu/~srinivas/">Prof. Srinivas Narasimhan</a>. Currently, I’m exploring topics in the domain of computational imaging and vision.
<br />
<br />
Before starting my Ph.D., I was a Research Scholar at <a href="http://www.nec-labs.com">NEC Labs America</a>, <a href="http://www.nec-labs.com/research-departments/media-analytics/media-analytics-home">Media Analytics</a> department where I worked with <a href="https://cseweb.ucsd.edu/~mkchandraker/">Prof. Manmohan Chandraker</a> on problems related to trajectory prediction and Embodied AI.
<br />
<br />
In the past, I was associated with <a href="https://robotics.iiit.ac.in/">Robotics Research Center</a> where I worked with <a href="https://www.iiit.ac.in/people/faculty/mkrishna/">Prof. K. Madhava Krishna</a> and <a href="https://faculty.iiit.ac.in/~vgandhi/">Prof. Vineet Gandhi</a>. I spent the summer 2017 at <a href="https://www.iiitd.ac.in">IIIT Delhi</a>, working with <a href="https://sites.google.com/view/sanjitkkaul/">Prof. Sanjit Kaul</a> where I first experienced the thrill of robotics by building a self-driving vehicle prototype.
<br />
<br />
I’m always happy to talk about research and other things. Please feel free to reach me via email at nnsriram-at-cmu dot edu.</p>

<!-- <div class="quote--container">
    <p class="quote">
        Never give up on a dream just because of the time it will take to accomplish it. The time will pass anyway.
    </p>
    <p class="quote--author">&ndash; Earl Nightingale</p>
</div>
 -->
<hr />

<font size="4">
<div align="center"><b>News</b></div>
</font>

<!-- <img src="/images/new.jpeg" algin="left" width="45" />  -->
<p><strong>08/22:</strong> Moved to Pittsburgh and started my PhD at <a href="https://www.ri.cmu.edu">RI, CMU</a>. <br />
<strong>10/21:</strong> Talk on <a href="https://youtu.be/ICSqcqq1sn0?t=120"><strong>Predicting simultaneous multi-hypotheses futures</strong></a> at Robotics Research Group, IIT BHU<br />
<strong>02/21:</strong> Paper <em>Divide and Conquer for Lane-Aware Diverse Trajectory Prediction</em> is accepted to <strong>CVPR 2021 (Oral)</strong>. <br />
<!-- **00/21:** Reviewer for CVPR 2021, IROS 2021, ICCV 2021 and CVPR 2022 (Invited)<br/> -->
<strong>07/20:</strong> Paper <em>SMART: Simultaneous Multi-Agent Recurrent Trajectory Prediction</em> is accepted to ECCV 2020. <br />
<!-- **11/19:** Served as a reviewer for ICRA 2020. <br/> -->
<strong>07/19:</strong> Joined NEC Labs America as a Research Scholar. <br />
<strong>06/19:</strong> Paper <em>Talk to the Vehicle: Language Conditioned Autonomous Navigation</em> is accepted at IROS 2019. <br />
<strong>04/19:</strong> Paper <em>A Hierarchical Network for Diverse Trajectory Proposals</em> is accepted at IV 2019. <br />
<strong>04/19:</strong> Work on shrinking domain based control for planning is accepted at AIR 2019. <br /></p>

<hr />

<font size="4">
<div align="center"><b>Publications</b></div> <br />
</font>

<table style="background-color:#F1F7FC">
  <tr>
    <td style="text-align:left;vertical-align:top;padding-top:1%;padding-left:1%;" width="25%">    <img src="/images/publications/LongHOT.png" align="left" width="250" />    </td>
    <td width="70%">    <a href="https://arxiv.org/pdf/2210.15908.pdf"> 
      <strong> Long-HOT: A Modular Hierarchical Approach for Long-Horizon Object Transport</strong> </a> <br /> 
      <strong>Sriram Narayanan</strong>, Dinesh Jayaraman, Manmohan Chandraker<br />
      <span style="color: green">ArXiv 2022 </span> <strong></strong><br />
      <a class="links" href="https://arxiv.org/pdf/2210.15908.pdf"> Paper </a>
      <a class="links" href="https://youtu.be/0f9UpmDfSCs"> Video </a>
      <div class="readmore">
        <p align="justify">We address key challenges in long-horizon embodied exploration and navigation by proposing a new object transport task and a novel modular framework for temporally extended navigation. Our first contribution is the design of a novel Long-HOT environment focused on deep exploration and long-horizon planning where the agent is required to efficiently find and pick up target objects to be carried and dropped at a goal location, with load constraints and optional access to a container if it finds one. Further, we propose a modular hierarchical transport policy (HTP) that builds a topological graph of the scene to perform exploration with the help of weighted frontiers. Our hierarchical approach uses a combination of motion planning algorithms to reach point goals within explored locations and object navigation policies for moving towards semantic targets at unknown locations. Experiments on both our proposed Habitat transport task and on MultiOn benchmarks show that our method significantly outperforms baselines and prior works. Further, we validate the effectiveness of our modular approach for long-horizon transport by demonstrating meaningful generalization to much harder transport scenes with training only on simpler versions of the task.</p>
        <span class="readmore-link"></span>
      </div>
    </td> 
  </tr>
</table>

<table style="background-color:#F1F7FC">
  <tr>
    <td style="text-align:left;vertical-align:top;padding-top:1%;padding-left:1%;" width="25%">    <img src="/images/publications/DAC_cvpr21.png" align="left" width="250" />    </td>
    <td width="70%">    <a href="https://arxiv.org/abs/2104.08277"> 
      <strong> Divide-and-Conquer for Lane-Aware Diverse Trajectory Prediction</strong> </a> <br /> 
      <strong>Sriram Narayanan</strong>, Ramin Moslemi, Francesco Pittaluga, Buyu Liu, Manmohan Chandraker<br />
      <span style="color: green">Computer Vision and Pattern Recognition (CVPR), 2021 </span> <strong><span style="color: red">(Oral)</span></strong><br />
      <a class="links" href="https://arxiv.org/abs/2104.08277"> Paper </a>
      <a class="links" href="https://www.nec-labs.com/research-departments/media-analytics/blog/Divide-and-Conquer-for-Lane-Aware-Diverse-Trajectory-Prediction"> Blog </a>
      <a class="links" href="https://www.youtube.com/watch?v=PKFggOWN638"> Talk </a>&nbsp; 
      <div class="readmore">
        <!-- <p align='justify'>We propose a novel multi-choice objective called Divide-And-Conquer (DAC) for diverse future prediction. Our approach acts as a better initialization technique to winner-takes-all objective, resulting in diverse outputs without any spurious modes. Further, we present a novel trajectory prediction framework called ALAN that uses existing lane centerlines as anchors to provide scene consistent predictions</p> -->
        <p align="justify">Trajectory prediction is a safety-critical tool for autonomous vehicles to plan and execute actions. Our work addresses two key challenges in trajectory prediction, learning multimodal outputs, and better predictions by imposing constraints using driving knowledge. Recent methods have achieved strong performances using Multi-Choice Learning objectives like winner-takes-all (WTA) or best-of-many. But the impact of those methods in learning diverse hypotheses is under-studied as such objectives highly depend on their initialization for diversity. As our first contribution, we propose a novel Divide-And-Conquer (DAC) approach that acts as a better initialization technique to WTA objective, resulting in diverse outputs without any spurious modes. Our second contribution is a novel trajectory prediction framework called ALAN that uses existing lane centerlines as anchors to provide trajectories constrained to the input lanes. Our framework provides multi-agent trajectory outputs in a forward pass by capturing interactions through hypercolumn descriptors and incorporating scene information in the form of rasterized images and per-agent lane anchors. Experiments on synthetic and real data show that the proposed DAC captures the data distribution better compare to other WTA family of objectives. Further, we show that our ALAN approach provides on par or better performance with SOTA methods evaluated on Nuscenes urban driving benchmark.</p>
        <span class="readmore-link"></span>
      </div>
    </td> 
  </tr>
</table>

<table style="background-color:#F1F7FC">
  <tr>
    <td style="text-align:left;vertical-align:top;padding-top:1%;padding-left:1%;" width="25%">    <img src="/images/publications/eccv20.gif" align="left" width="250" />    </td>
    <td width="70%">    <a href="https://arxiv.org/pdf/2007.13078.pdf"> 
      <strong> SMART: Simultaneous Multi-Agent Recurrent Trajectory Prediction</strong> </a> <br /> 
      <strong>Sriram Narayanan</strong>, Buyu Liu, Francesco Pittaluga, Manmohan Chandraker<br />
      <span style="color: green">European Conference on Computer Vision (ECCV), 2020 </span><br />
      <a class="links" href="https://arxiv.org/pdf/2007.13078.pdf"> Paper </a>
      <a class="links" href="https://youtu.be/puBAVMoLgQU"> Video </a>
      <a class="links" href="http://www.nec-labs.com/uploads/Documents/Media-Analytics/research-videos/SMART-%20Simultaneous%20Multi-Agent%20Recurrent%20Trajectory%20Prediction.mp4"> Short Talk </a>&nbsp; 
      <div class="readmore">
        <!-- <p align='justify'> A convLSTM with novel state pooling operations and losses to predict scene-consistent states of multiple agents in a single forward pass and a novel method that generates diverse predictions while accounting for scene semantics and multi-agent interactions, with constant-time inference independent of the number of agents.</p> -->
        <p align="justify">We propose advances that address two key challenges in future trajectory prediction: (i) multimodality in both training data and predictions and (ii) constant time inference regardless of number of agents. Existing trajectory predictions are fundamentally limited by lack of diversity in training data, which is difficult to acquire with sufficient coverage of possible modes. Our first contribution is an automatic method to simulate diverse trajectories in the top-view. It uses pre-existing datasets and maps as initialization, mines existing trajectories to represent realistic driving behaviors and uses a multi-agent vehicle dynamics simulator to generate diverse new trajectories that cover various modes and are consistent with scene layout constraints. Our second contribution is a novel method that generates diverse predictions while accounting for scene semantics and multi-agent interactions, with constant-time inference independent of the number of agents. We propose a convLSTM with novel state pooling operations and losses to predict scene-consistent states of multiple agents in a single forward pass, along with a CVAE for diversity. We validate our proposed multi-agent trajectory prediction approach by training and testing on the proposed simulated dataset and existing real datasets of traffic scenes. In both cases, our approach outperforms SOTA methods by a large margin, highlighting the benefits of both our diverse dataset simulation and constant-time diverse trajectory prediction methods.</p>
        <span class="readmore-link"></span>
      </div>
    </td> 
  </tr>
</table>

<table style="background-color:#F1F7FC">
  <tr>
    <td style="text-align:left;vertical-align:top;padding-top:1%;padding-left:1%;" width="25%">    <img src="/images/publications/iros19.gif" align="left" width="250" />    </td>
    <td width="70%">    <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8967929"> 
    <strong> Talk to the Vehicle: Language Conditioned Autonomous Navigation of Self Driving Cars</strong> </a> <br /> 
     <strong>Sriram Narayanan</strong>*, Tirth Maniar*, Jayaganesh Kalyanasundaram, Vineet Gandhi, Brojeshwar Bhowmick, K Madhava Krishna<br />
     <span style="color: green">International Conference on Intelligent Robots and Systems (IROS), 2019</span><br />
    <a class="links" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8967929"> Paper </a>
    <a class="links" href="https://www.youtube.com/watch?v=zx8s2l2tcAU"> Video </a>&nbsp; 
    <div class="readmore">
        <p align="justify">We propose a novel pipeline that blends encodings from natural language and 3D semantic maps obtained from visual imagery to generate local trajectories that are executed by a low-level controller. The pipeline precludes the need for a prior registered map through a local waypoint generator neural network. The waypoint generator network (WGN) maps semantics and natural language encodings (NLE) to local waypoints. A local planner then generates a trajectory from the ego location of the vehicle (an outdoor car in this case) to these locally generated waypoints while a low-level controller executes these plans faithfully. The efficacy of the pipeline is verified in the CARLA simulator environment as well as on local semantic maps built from real-world KITTI dataset. In both these environments (simulated and real-world) we show the ability of the WGN to generate waypoints accurately by mapping NLE of varying sequence lengths and levels of complexity. We compare with baseline approaches and show significant performance gain over them. And finally, we show real implementations on our electric car verifying that the pipeline lends itself to practical and tangible realizations in uncontrolled outdoor settings. In loop execution of the proposed pipeline that involves repetitive invocations of the network is critical for any such language-based navigation framework. This effort successfully accomplishes this thereby bypassing the need for prior metric maps or strategies for metric level localization during traversal.</p>
        <span class="readmore-link"></span>
      </div>
    </td> 
  </tr>
</table>

<table style="background-color:#F1F7FC">
  <tr>
    <td style="text-align:left;vertical-align:top;padding-top:1%;padding-left:1%;" width="25%">    <img src="/images/publications/iv19.gif" align="left" width="250" />    </td>
    <td width="70%">    <a href="https://ieeexplore.ieee.org/abstract/document/8813986"> 
    <strong> A Hierarchical Network for Diverse Trajectory Proposals </strong> </a> <br /> 
    <strong>Sriram Narayanan</strong>, Gourav Kumar, Abhay Singh, M. Siva Karthik, Saket Saurav, Brojeshwar Bhowmick, K. Madhava Krishna<br /> 
    <span style="color: green">Intelligent Vehicles Symposium (IV), 2019</span> <br />
    <a class="links" href="https://ieeexplore.ieee.org/abstract/document/8813986"> Paper </a> 
    <a class="links" href="https://www.youtube.com/watch?v=cvq2dFS-dZo"> Video </a> &nbsp; 
    <div class="readmore">
        <p align="justify">Autonomous explorative robots frequently encounter scenarios where multiple future trajectories can be pursued. Often these are cases with multiple paths around an obstacle or trajectory options towards various frontiers. Humans in such situations can inherently perceive and reason about the surrounding environment to identify several possibilities of either manoeuvring around the obstacles or moving towards various frontiers. In this work, we propose a 2 stage Convolutional Neural Network architecture which mimics such an ability to map the perceived surroundings to multiple trajectories that a robot can choose to traverse. The first stage is a Trajectory Proposal Network which suggests diverse regions in the environment which can be occupied in the future. The second stage is a Trajectory Sampling network which provides a finegrained trajectory over the regions proposed by Trajectory Proposal Network. We evaluate our framework in diverse and complicated real life settings. For the outdoor case, we use the KITTI dataset and our own outdoor driving dataset. In the indoor setting, we use an autonomous drone to navigate various scenarios and also a ground robot which can explore the environment using the trajectories proposed by our framework. Our experiments suggest that the framework is able to develop a semantic understanding of the obstacles, open regions and identify diverse trajectories that a robot can traverse. Our comparisons portray the performance gain of the proposed architecture over a diverse set of methods against which it is compared.</p>
        <span class="readmore-link"></span>
      </div>
    </td> 
  </tr>
</table>

<table style="background-color:#F1F7FC">
  <tr>
    <td style="text-align:left;vertical-align:top;padding-top:1%;padding-left:1%;" width="25%">   <img src="/images/publications/air19.gif" align="left" width="250" />    </td>
    <td width="70%">    <a href="https://arxiv.org/pdf/1804.08679.pdf"> 
    <strong> Gradient Aware - Shrinking Domain based Control Design for Reactive Planning Frameworks used in Autonomous Vehicles </strong> </a> <br /> 
     Adarsh Modh, Siddharth Singh, A. V. S. Sai Bhargav Kumar, <strong>Sriram Narayanan</strong>, K. Madhava Krishna<br /> 
    <span style="color: green">Proceedings of the Advances in Robotics (AIR), 2019</span> <br />
    <a class="links" href="https://arxiv.org/pdf/1804.08679.pdf"> Paper </a> <a class="links" href="https://www.youtube.com/watch?v=Yf4F0dvkwQE"> Video </a> &nbsp; 
    <div class="readmore">
        <p align="justify">In this paper, we present a novel control law for longitudinal speed control of autonomous vehicles. The key contributions of the proposed work include the design of a control law that reactively integrates the longitudinal surface gradient of road into its operation. In contrast to the existing works, we found that integrating the path gradient into the control framework improves the speed tracking efficacy. Since the control law is implemented over a shrinking domain scheme, it minimizes the integrated error by recomputing the control inputs at every discretized step and consequently provides less reaction time. This makes our control law suitable for motion planning frameworks that are operating at high frequencies. Furthermore, our work is implemented using a generalized vehicle model and can be easily extended to other classes of vehicles. The performance of gradient aware-shrinking domain based controller is implemented and tested on a stock electric vehicle on which a number of sensors are mounted. Results from the tests show the robustness of our control law for speed tracking on a terrain with varying gradient while also considering stringent time constraints imposed by the planning framework. </p>
        <span class="readmore-link"></span>
      </div>
    </td> 
  </tr>
</table>

<hr />

<font size="4">
<div align="center"><b>Professional Services</b></div> <br />
<p align="justify"> Conference Reviewer: ICRA 2020, IROS 2021, CVPR 2021, ICCV 2021, ICRA 2022, CVPR 2022, ECCV 2022, AAAI 2022</p>
</font>

<hr />

<font size="4">
<div align="center"><b>Published Patents</b></div> <br />
</font>

<table style="background-color:#F1F7FC">
  <tr>
    <td style="text-align:left;vertical-align:top;padding-top:1%;padding-left:1%;" width="25%">    <img src="images/publications/dac_patent.png" align="left" width="250" />    </td>
    <td width="70%">    <a href="https://patentimages.storage.googleapis.com/eb/13/4c/7c790a0183804d/US20220144256A1.pdf"> 
    <strong> Divide-and-conquer for lane-aware diverse trajectory prediction</strong> </a> <br /> 
      <strong> Sriram Narayanan </strong>, Ramin Moslemi, Francesco Pittaluga, Buyu Liu, Manmohan Chandraker<br /> 
     <i>US Patent App. 17/521,139</i><br />
    <a class="links" href="https://patentimages.storage.googleapis.com/eb/13/4c/7c790a0183804d/US20220144256A1.pdf"> Patent </a>&nbsp; 
    </td> 
  </tr>
</table>

<table style="background-color:#F1F7FC">
  <tr>
    <td style="text-align:left;vertical-align:top;padding-top:1%;padding-left:1%;" width="25%">    <img src="/images/publications/smart_patent.png" align="left" width="250" />    </td>
    <td width="70%">    <a href="https://patentimages.storage.googleapis.com/8a/e8/aa/656d776c87703f/US20210276547A1.pdf"> 
    <strong> Multi-agent trajectory prediction</strong> </a> <br /> 
      <strong> Sriram Narayanan </strong>, Buyu Liu, Ramin Moslemi, Francesco Pittaluga, Manmohan Chandraker<br /> 
     <i>US Patent App. 17/187,157</i><br />
    <a class="links" href="https://patentimages.storage.googleapis.com/8a/e8/aa/656d776c87703f/US20210276547A1.pdf"> Patent </a>&nbsp; 
    </td> 
  </tr>
</table>

<table style="background-color:#F1F7FC">
  <tr>
    <td style="text-align:left;vertical-align:top;padding-top:1%;padding-left:1%;" width="25%">    <img src="/images/publications/diversity_matters_patent.png" align="left" width="250" />    </td>
    <td width="70%">    <a href="https://patentimages.storage.googleapis.com/1e/34/60/2fd7af876b8ae1/US20210148727A1.pdf"> 
    <strong> Simulating diverse long-term future trajectories in road scenes</strong> </a> <br /> 
      <strong> Sriram Narayanan </strong>, Manmohan Chandraker<br /> 
     <i>US Patent App. 17/090,399</i><br />
    <a class="links" href="https://patentimages.storage.googleapis.com/1e/34/60/2fd7af876b8ae1/US20210148727A1.pdf"> Patent </a>&nbsp; 
    </td> 
  </tr>
</table>

<table style="background-color:#F1F7FC">
  <tr>
    <td style="text-align:left;vertical-align:top;padding-top:1%;padding-left:1%;" width="25%">    <img src="/images/publications/iv_patent.png" align="left" width="250" />    </td>
    <td width="70%">    <a href="https://patentimages.storage.googleapis.com/06/60/81/2f29a5dd03089e/US20200387163A1.pdf"> 
    <strong> Method and a system for hierarchical network based diverse trajectory proposal</strong> </a> <br /> 
      Brojeshwar Bhowmick, K. Madhava Krishna, <strong> Sriram Narayanan</strong>, Gourav Kumar, Abhay Singh, M. Siva Karthik, Saket Saurav<br /> 
     <i>US Patent App. 16/894,411</i><br />
    <a class="links" href="https://patentimages.storage.googleapis.com/06/60/81/2f29a5dd03089e/US20200387163A1.pdf"> Patent </a>&nbsp; 
    </td> 
  </tr>
</table>

<hr />

<font size="4">
<div align="center"><b>Bachelor's Projects</b></div> <br />
</font>

<table style="background-color:#F1F7FC">
  <tr>
    <td style="text-align:left;vertical-align:top;padding-top:1%;padding-left:1%;" width="25%">    <img src="/images/swarath_gif_resize.gif" align="left" width="200" />    </td>
    <!-- <td width="70%">    <a href="https://nnsriram.weebly.com/swarath.html"> 
    <strong> Swarath - Self driving Car </strong> </a> <br/>  -->
    <td width="70%">
    <strong style="color:blue;font-size:18px;"><a href="https://nnsriram.weebly.com/swarath.html"> Swarath - Autonomous Car Project </a></strong><br />
  Internship project at IIIT Delhi during Summer 2017, where I worked on building a SDV prototype. It started with designing controller and planner for the vehicle. Later built perception systems where sensor outputs were used to obtain occupancies. Further, integrated the planner and perception modules together. The internship ended with demonstrating autonomous navigation with GPS waypoints as input. <br />
    <a class="links" href="https://www.youtube.com/watch?v=z3FdsK_w_Nw"> Video </a> &nbsp;
    </td> 
  </tr>
</table>

<!-- <hr> -->

<table style="background-color:#F1F7FC">
<tr>
    <td style="text-align:left;vertical-align:top;padding-top:1%;padding-left:1%;" width="25%">    <img src="/images/btech_thesis_crop.gif" align="left" width="200" />    </td>
    <!-- <td width="70%">    <a href="https://nnsriram.weebly.com/swahana.html"> 
    <strong> Building Perception, Planning and Control for Autonomous Vehicles </strong> </a> <br/>  -->
    <td width="70%">
    <strong style="color:blue;font-size:18px;"><a href="https://nnsriram.weebly.com/swahana.html"> Building Perception, Planning and Control for Autonomous Vehicles </a></strong><br />
  Bachelor thesis at IIIT Hyderabad, started with designing a low-level controller for a stock car from scratch. Built controllers for throttle, steering and brakes without CAN support. Further integrated perception (ORB-SLAM, LOAM) and planning (RRT*) modules. Demonstrated static obstacle avoidance in tight spaces which was showcased at Research and Development showcase held at IIIT. A part of my thesis culminated into a paper published at AIR 19. <br />
    <a class="links" href="https://drive.google.com/file/d/1CAmw-Pam0xOWf-3rUHwM1NNhlOCeX9a2/view"> Report </a> <a class="links" href="https://www.youtube.com/watch?v=Fs-Pt0M1j5I"> Video </a> &nbsp;
    </td> 
  </tr>
</table>

<!-- <hr> -->
<table style="background-color:#F1F7FC">
<tr>
    <td style="text-align:left;vertical-align:top;padding-top:1%;padding-left:1%;" width="25%">    <img src="/images/projects/collage_bs.png" align="left" width="200" />    </td>
    <td width="70%">
    <strong style="color:blue;font-size:18px;"><a href="https://drive.google.com/file/d/14Q1tsOjkWNgl8JsLJuONBHBynsyhAjAG/view"> Background Subtraction </a></strong><br />
  Project work on Background Subtraction with Dr. Manigandan during bachelors. Designed a background subtraction algorithm based on mixture models.<br />
  <!-- Designed a novel Background Subtraction algorithm based on univariate Gaussians. Obtained an improved F-Measure of 0.8221 and percentage of false classification as low as 2.6%. Implemented various traditional BS techniques as baselines for comparison.<br> -->
    <a class="links" href="https://drive.google.com/file/d/14Q1tsOjkWNgl8JsLJuONBHBynsyhAjAG/view"> Paper </a> &nbsp;
    </td> 
  </tr>
</table>

<!-- <hr> -->
<table style="background-color:#F1F7FC">
<tr>
    <!-- <td width="20%">    <iframe align="left" width="180" src="https://www.youtube.com/embed/EyNALTEMpUo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> </td> -->
    <td style="text-align:left;vertical-align:top;padding-top:1%;padding-left:1%;" width="25%">    <img src="/images/eyantra_resize.gif" align="left" width="200" />    </td>
    <!-- <td width="5%"></td> -->
    <td width="70%">
    <strong style="color:blue;font-size:18px;"><a href="https://nnsriram.weebly.com/bothoven.html"> Bothoven - Eyantra Robotics Competition </a></strong><br />
  A multi-robot task execution problem, where two robots simultaneously plan paths, coordinate and execute a task of playing an audio file in a concurrent fashion. First FFT is applied to decode the notes from an audio file and is fed as input to the robots. The robots are aware of the map but are unaware of the obstacles present on the map. The map is updated and replanning is done when any one of them encounters an obstacle.<br />
    <a class="links" href="https://www.youtube.com/watch?v=EyNALTEMpUo"> Video </a> &nbsp;
    </td> 
  </tr>
</table>

<!-- <hr> -->
<table style="background-color:#F1F7FC">
<tr>
    <td style="text-align:left;vertical-align:top;padding-top:1%;padding-left:1%;" width="25%">    <img src="/images/projects/husky.png" align="left" width="200" />    </td>
    <td width="70%">
    <strong style="color:blue;font-size:18px;"><a href="https://nnsriram.weebly.com/husky.html"> Husky - Visual perception based Autonomous bot </a></strong><br />
  Implemented obstacle avoidance and waypoint following in outdoor and indoor scenarios. Sensors like stereo camera, GPS, IMU and wheel encoders were used. Occupancy map was generated upon which planning was done using RRT for the bot to reach the goal autonmously.<br />
    <a class="links" href="https://www.youtube.com/watch?v=XOhP3XEvSok"> Video </a> &nbsp;
    </td> 
  </tr>
</table>

<p><br /></p>

<hr />

<font size="2">
<div align="center"><b>I am privileged to be associated with the following</b></div>
<table text-align="center">
<tr>
  <td align="center"><a href="https://www.iiitd.ac.in/"><img src="images/iiit_d.png" width="70" /></a></td>  
  <td align="center"><a href="http://iiit.ac.in"><img src="images/iiit_h.png" width="80" /></a></td>  
  <td align="center"><a href="http://www.nec-labs.com/research-departments/media-analytics/media-analytics-home"><img src="images/nec_logo.png" width="70" /></a></td>  
  <td align="center"><a href="https://www.ri.cmu.edu/"><img src="images/cmu_logo.png" width="70" /></a></td>  
</tr>

<tr>
  <td align="center">Intern</td>
  <td align="center">RA</td>
  <td align="center">Research Scholar</td>
  <td align="center">PhD</td>
</tr>

<tr>
  <td align="center">Summer 2017</td>
  <td align="center">Dec 2017 - Jul 2019</td>
  <td align="center">Jul 2019 - Jul 2022</td>
  <td align="center">August 2022 - Present</td>
</tr>

</table>
</font>
<hr />


    </div><!-- /.article-wrap -->
    
  </article>
</div><!-- /#index -->

<div class="footer-wrap">
  <footer>
    

<span>&copy; 2023 . Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="http://mademistakes.com/minimal-mistakes/" rel="nofollow">Minimal Mistakes</a> theme. Template inspired from <a href="http://sivakm.github.io" rel="nofollow">Siva Karthik</a>. <script type="text/javascript" src="//rf.revolvermaps.com/0/0/3.js?i=5mz04hbjxkb&amp;b=0&amp;s=20&amp;m=1&amp;cl=00fff6&amp;co=000000&amp;cd=ff0000&amp;v0=60&amp;v1=60&amp;r=1" async="async"></script></span>

  </footer>
</div><!-- /.footer-wrap -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://localhost:4002/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://localhost:4002/assets/js/scripts.min.js"></script>

          
<style>

body{ 
  mmin-height:1000px;
}

.container {
  display: flex;
  justify-content: center;
}

.section {
  margin-right:20px;
}

.readmore {
  position: relative;
  max-height: 100px;
  overflow: hidden;
  width:95%;
  /*border: solid 1px blue;*/
  padding: 10px;
  margin-bottom: 20px;

  transition:max-height 0.15s ease-out;

}

.readmore.expand{
  max-height: 5000px !important;
  transition:max-height 0.35s ease-in-out;
}

.readmore-link{
  position: absolute;
  bottom: 0;
  right: 0;
  display: block;
  width:100%;
  height: 60px;
  text-align: center;
  color: blue;
  font-weight:bold;
  font-size:16px;
  padding-top:40px;
  background-image: linear-gradient(to bottom, transparent, #F1F7FC);
  cursor: pointer;
}

.readmore-link.expand {
  position: relative;
  background-image: none;
  padding-top:10px;
  height:20px;
}

.readmore-link:after {
  content:"Read more";
}
.readmore-link.expand:after{
  content:"Read less";
}

</style>

<script type="text/javascript">
  $(".readmore-link").click( function(e) {
  // record if our text is expanded
  var isExpanded =  $(e.target).hasClass("expand");
  
  //close all open paragraphs
  $(".readmore.expand").removeClass("expand");
  $(".readmore-link.expand").removeClass("expand");
  
  // if target wasn't expand, then expand it
  if (!isExpanded){
    $( e.target ).parent( ".readmore" ).addClass( "expand" );
    $(e.target).addClass("expand");  
  } 
});
</script>
</body>


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-59912294-1', 'auto');
  ga('send', 'pageview');

</script>
</html>